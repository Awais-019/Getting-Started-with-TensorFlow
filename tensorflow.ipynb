{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part1: Inroduction\n",
    "\n",
    "### 1.1: Installation\n",
    "TensorFlow is an open-source machine learning framework developed by Google. It allows developers to build and train machine learning models using a data flow graph that represents computation. TensorFlow is versatile and widely used for various machine learning tasks, including computer vision and natural language processing. It also includes a high-level API called Keras for quick and easy model building and training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow\n",
    "import tensorflow as tf"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Tensors\n",
    "In machine learning and TensorFlow, a tensor is a multi-dimensional array of numerical values that represents input data, intermediate computations, and output data in a neural network. Each tensor has a data type (e.g., float32 or int64) and a shape, which defines the number of dimensions and the size of each dimension. Tensors can be created using the `tf.constant()` function and manipulated using a wide range of mathematical operations. Tensors are a critical component of machine learning and TensorFlow, providing a flexible and efficient way to represent and manipulate data in a neural network."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.1 0-d Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`city` is a 0-d tensor\n",
      "`pi` is a 0-d tensor\n"
     ]
    }
   ],
   "source": [
    "city = tf.constant(\"Beijing\", tf.string)\n",
    "pi = tf.cos(tf.constant(3.1415926, tf.float32))\n",
    "\n",
    "print(\"`city` is a {}-d tensor\".format(tf.rank(city).numpy()))\n",
    "print(\"`pi` is a {}-d tensor\".format(tf.rank(pi).numpy()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.2 1-d Tensors\n",
    "Vectors and lists can create 1-d Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "`cities` is a 1-d tensor with shape [3]\n",
      "`fvrt_numbers` is a 1-d tensor with shape [2]\n"
     ]
    }
   ],
   "source": [
    "cities = tf.constant([\"Beijing\", \"Shanghai\", \"Guangzhou\"], tf.string)\n",
    "fvrt_numbers = tf.constant([3.1415926, 2.71828], tf.float32)\n",
    "\n",
    "print(\"`cities` is a {}-d tensor with shape {}\".format(tf.rank(cities).numpy(), tf.shape(cities)))\n",
    "print(\"`fvrt_numbers` is a {}-d tensor with shape {}\".format(tf.rank(fvrt_numbers).numpy(), tf.shape(fvrt_numbers)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2.3 Higher Dimensional Tensors\n",
    "\n",
    "You can create 2-d, 3-d 4-d and even higher order Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "matrix is a 2-d tensor with shape [2 3]\n",
      "data is a 3-d tensor with shape [ 10 256 256]\n",
      "images is a 4-d tensor with shape [ 10 256 256   3]\n"
     ]
    }
   ],
   "source": [
    "# a 2-d tensor\n",
    "matrix = tf.constant([[1, 2, 3], [4, 5, 6]], tf.int32)\n",
    "\n",
    "# a 3-d tensor\n",
    "data = tf.ones([10, 256, 256], tf.int32)\n",
    "\n",
    "# a 4-d tensor\n",
    "images = tf.zeros([10, 256, 256, 3], tf.int32)\n",
    "\n",
    "print(\"matrix is a {}-d tensor with shape {}\".format(tf.rank(matrix).numpy(), tf.shape(matrix)))\n",
    "print(\"data is a {}-d tensor with shape {}\".format(tf.rank(data).numpy(), tf.shape(data)))\n",
    "print(\"images is a {}-d tensor with shape {}\".format(tf.rank(images).numpy(), tf.shape(images)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Computations on Tensors\n",
    "In TensorFlow, computations occur in tensors through a process called data flow graph computation. A data flow graph is a directed graph where the nodes represent mathematical operations and the edges represent the tensors that flow between them. When a TensorFlow program is executed, it first builds a data flow graph, which specifies the operations to be performed and the order in which they should be executed.\n",
    "\n",
    "Once the data flow graph is constructed, TensorFlow performs a computation called a forward pass, which propagates the input data through the graph to produce an output. During the forward pass, each node in the graph performs its operation on the input tensors it receives and passes the output tensor(s) to the next node(s) in the graph. This process continues until the output tensor(s) are produced by the final node in the graph.\n",
    "\n",
    "After the forward pass is completed, TensorFlow performs a backward pass, which calculates the gradients of the output tensor(s) with respect to the input tensor(s). These gradients are used in a process called backpropagation, which updates the weights of the neural network to minimize the loss function.\n",
    "\n",
    "Overall, computations in TensorFlow occur in tensors through a data flow graph computation process that involves a forward pass and a backward pass. This process allows TensorFlow to efficiently perform complex mathematical operations on large datasets and train sophisticated machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(76, shape=(), dtype=int32)\n",
      "tf.Tensor(76, shape=(), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "# create nodes in a graph\n",
    "a = tf.constant(15, name=\"a\")\n",
    "b = tf.constant(61, name=\"b\")\n",
    "\n",
    "# add them\n",
    "c1 = tf.add(a, b, name=\"c1\")\n",
    "c2 = a + b # TensorFlow overrides the \"+\" operation so that it is able to act on tensors\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part2: Neural Networks in TF"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Defining Neural Networks\n",
    "TensorFlow provides a high level API `Keras` to define neural networks."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1 Defining a custom Layer\n",
    "This is defining a custom dense layer in TensorFlow using the `tf.keras.layers.Layer` class as a base. The `__init__` method initializes the number of output nodes for the layer. The `build` method creates the weights for the layer, which are the weight matrix `W` and bias vector `b`. The `call` method performs the forward pass of the layer, which involves matrix multiplication of the input `x` with the weight matrix `W`, adding the bias vector `b`, and applying the sigmoid activation function to the result.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.7276925  0.38845906 0.21887647]], shape=(1, 3), dtype=float32)\n",
      "[<tf.Variable 'weight:0' shape=(2, 3) dtype=float32, numpy=\n",
      "array([[ 0.39757   , -0.07050097, -0.2558784 ],\n",
      "       [ 1.0046954 ,  0.12365389, -0.43444103]], dtype=float32)>, <tf.Variable 'bias:0' shape=(1, 3) dtype=float32, numpy=array([[-0.41931885, -0.5069469 , -0.58190644]], dtype=float32)>]\n"
     ]
    }
   ],
   "source": [
    "class OurDenseLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_output_nodes):\n",
    "        super(OurDenseLayer, self).__init__()\n",
    "        self.n_output_nodes = n_output_nodes\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        d = int(input_shape[-1])\n",
    "        self.W = self.add_weight(\"weight\", shape=[d, self.n_output_nodes])\n",
    "        self.b = self.add_weight(\"bias\", shape=[1, self.n_output_nodes])\n",
    "\n",
    "    def call(self, x):\n",
    "        z = tf.matmul(x, self.W) + self.b\n",
    "        y = tf.sigmoid(z)\n",
    "        return y\n",
    "    \n",
    "\n",
    "\n",
    "# Since layer parameters are initialized lazily upon first call, you must\n",
    "# call the layer at least once before using inspecting the weights\n",
    "# (otherwise no weights will have been created)\n",
    "layer = OurDenseLayer(3)\n",
    "layer.build((1, 2))\n",
    "print(layer.call(tf.ones([1, 2]).numpy()))\n",
    "print(layer.trainable_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2 Defining a neural network using Sequential API\n",
    "The `tf.keras.Sequential` class provides a simple way to build a neural network with a single stack of layers. The `Sequential` constructor takes a list of layer instances, which are stacked together to build the neural network. The `add` method can also be used to add layers to the neural network. The `summary` method prints a summary of the neural network architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "n_output_nodes = 3\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "dense_layer = Dense(n_output_nodes, activation=\"sigmoid\")\n",
    "model.add(dense_layer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.30796766 0.26908135 0.59257346]]\n"
     ]
    }
   ],
   "source": [
    "x_input = tf.constant([[1, 2.]], shape=(1, 2))\n",
    "\n",
    "model_output = model(x_input).numpy()\n",
    "print(model_output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.3 Defining a neural network using Subclassing API\n",
    "The `tf.keras.Model` class provides a more flexible way to build a neural network by subclassing the `Model` class and defining the forward pass through the `call` method. The `__init__` method initializes the layers of the neural network. The `call` method performs the forward pass, which involves passing the input `x` through each layer in the neural network in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "\n",
    "class SubClassModel(tf.keras.Model):\n",
    "    def __init__(self, n_output_nodes):\n",
    "        super(SubClassModel, self).__init__()\n",
    "        self.dense_layer = Dense(n_output_nodes, activation=\"sigmoid\")\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return self.dense_layer(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[0.13677017 0.735966   0.48733085]], shape=(1, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x_input = tf.constant([[1, 2.]], shape=(1, 2))\n",
    "\n",
    "model = SubClassModel(3)\n",
    "\n",
    "print(model.call(x_input))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part3: Automatic Derviative with TF\n",
    "\n",
    "### 3.1 Automatic Differentiation\n",
    "Automatic differentiation is a technique for numerically evaluating the derivative of a function. In TensorFlow, automatic differentiation is performed using a process called reverse-mode differentiation, which is also known as backpropagation. In reverse-mode differentiation, the derivatives are calculated starting from the output of the neural network and working backwards through the computational graph. This allows TensorFlow to efficiently calculate the gradients of the loss with respect to many variables by making a single pass through the computational graph.\n",
    "`tf.GradientTape` is used to perform automatic differentiation in TensorFlow. To compute the gradient of a function, you can use `tf.GradientTape` in a `with` statement, and TensorFlow will automatically compute the gradient of any operations that occur inside the `with` statement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# Gradient computation with GradientTape\n",
    "x = tf.Variable(3.0)\n",
    "\n",
    "with tf.GradientTape() as tape:\n",
    "    y = x * x\n",
    "\n",
    "dy_dx = tape.gradient(y, x)\n",
    "print(dy_dx.numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
